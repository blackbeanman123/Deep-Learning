
1. 有的时候，当loss达到plateau之后，继续训练时loss会在一定范围内不断波动，难以收敛（且取决于更新方式，如validation loss小就保存参数，会过拟合到validation set的上面去）
2. pytorch只要不重新加载optimizer和model之类的，会继续使用之前的param
3. 多加注意参数的修改，修改参数后是否给到了optimizer和model，device的信息有没有给对（to(cuda)）
4.  一开始学习率都会调的很大，然后，学习率开始时应较快的下降，后来下降的应越来越慢
5. 不同数据集和初始值，调的lr可能大不相同
5.  有的时候不论怎么调小学习率training loss和validation loss都在一个固定点附近不下降了，这是因为该点是一个局部最低点，得调大学习率越过
6.  batch size 越小 loss的波动越大
7. learning rate 不要下调的太快，可能会正好错过好的学习率
8. 调学习率技巧：https://discuss.pytorch.org/t/adaptive-learning-rate/320/3
9. https://blog.ailemon.me/2019/02/26/solution-to-loss-doesnt-drop-in-nn-train/
10. https://github.com/audier/DeepSpeechRecognition/issues/95
11. https://www.jiqizhixin.com/articles/nn-learning-rate
12. https://zhuanlan.zhihu.com/p/78096138